---
title: "Current State of Writing Functions in Stan-Math"
author: "Stan Development Team"
date: "24 January 2021"
output: html_document
---

```{r setup, include=FALSE}
library("cmdstanr")
```

This is meant as a basic guide for writing and maintaining functions in the
[stan-dev/math](https://github.com/stan-dev/math) repository. This is the
automatic differentiation (autodiff) library behind the Stan language, and
the vast majority of the functions exposed at the Stan language level are
implemented here in C++.

In the course of the Math library's existance, C++ has changed substantially.
Math was originally written before C++11. It currently targets C++14. In the near
future it will transition to C++17. With this in mind, there are many different
ways to write Math functions. This guide tries to document best practices,
conventions which not all functions in Math follow, but should be followed for
new code to keep the code from getting unweildy (the old patterns will be
updated eventually).

The title contans "Current State" to emphasize that if any information here is
out of date or any advice does not work, it should be reported as a bug (the
[example-models](https://github.com/stan-dev/example-models)).

This document builds on the information in the
[Stan Math Library](https://arxiv.org/abs/1509.07164) paper.

# Writing a function

Basic functions in the Math library should be compatible with the autodiff
tools in Math. The most important of these is reverse mode autodiff (this
is what is used in Stan) but forward and mixed mode autodiff types are also
supported. The intention behind supporting these other types is that the
higher order derivatives will be useful for different algorithms in Stan.

Because reverse mode autodiff is the component used in Stan, this is by far
the most mature and well tested component of the Math library. Any new
function introduced to the Math library by default are expected to support
and be tested with the higher order automatic differentiation, but exceptions
have been made. In particular, the differential equations solvers
only support reverse mode autodiff (and there are a couple others).

Functions in the Math library are organized according to the different
autodiff tools they depend on.

Functions with no dependencies go in `stan/math/prim/fun`. Similarly
functions implemented with reverse mode autodiff dependences go in
`stan/math/rev/fun`. Functions with forward mode autodiff dependencies
go in `stan/math/fwd/fun`, and functions that depend on both autodiff
types go in `stan/mat/mix/fun`.

Note, many functions in `prim/fun` support all the different autodiff
modes -- these folders separate functions by what they depend on, not
what they support. Functions in `prim/fun` should be as generic as
possible, and the `rev/fun`, `fwd/fun`, and `mix/fun` implementations
should be used to:

1. Make the autodiff'd versions of a function faster
2. Improve the numerical behavior of certain derivatives
3. Implement autodiff for functions that cannot be easily autodiff'd
4. Implement autodiff using hardware that does not directly support the Stan
autodiff types

## Argument Types

Functions in Math are written to interact with code generated for Stan, which
means that the types in Stan map to Math types. The basic Stan types are `int`,
`real`, `vector`, `row_vector`, `matrix`, and then arrays of any of these types
(and arrays of arrays are allowed). The `vector`, `row_vector`, and `matrix`
types are implemented using the Eigen library under the hood.

The relationship between Stan types and types in Math is a one to many
because of all the different autodiff types and details of how Eigen expressions
work. In terms of the basic arithmetic types the mapping is fairly simple:

- `int` in Stan maps to `int` in C++
- `real` in Stan maps to `double` in C++
- `vector` in Stan maps to `Eigen::VectorXd`
- `row_vector` in Stan maps to `Eigen::RowVectorXd`
- `matrix` in Stan maps to `Eigen::MatrixXd`
- Any array type (`int[]`, `real[]`, or `T[]` for `T` any of the above types)
map to `std::vector<C>` types where `C` is the C++ equivalent of `T`.

Once reverse mode autodiff is included, there is a new type, `stan::math::var`
that is the autodiff replacement for `double`. Excluding the `stan::math` namespace
for brevity, this expands the list to:

- `int` in Stan maps to `int` in C++
- `real` in Stan maps to `double` or `var` in C++
- `vector` in Stan maps to `Eigen::VectorXd` and
`Eigen::Matrix<var, Eigen::Dynamic, 1>` in C++
- `row_vector` in Stan maps to `Eigen::RowVectorXd` and
`Eigen::Matrix<var, 1, Eigen::Dynamic>` in C++
- `matrix` in Stan maps to `Eigen::MatrixXd` and
`Eigen::Matrix<var, Eigen::Dynamic, Eigen::Dynamic>` in C++
- Any array type (`int[]`, `real[]`, or `T[]` for `T` any of the above types)
map to `std::vector<C>` types where `C` is the C++ equivalent of `T`.

This mapping will continue to get longer as forward mode autodiff types and
Eigen expressions are taken into account. For all intensive purposes, this list
is too long to write out.

Before diving into this though, it will be valuable to discuss in detail the
argument and return types from simple one and two argument functions.

### One Argument Functions (and computing a return type)

The Stan function `real sin(real)` expects the followng C++ functions to be
defined:

```cpp
double sin(double);
var sin(var);
```

Note, return types are not part of a C++ function signature. They are included
here because working autodiff makes assumptions about return types. For instance,
a function that accepts an autodiff type should return an autodiff type.
Similarly, the C++ signatures `double sin(const double&)`, or
`double sin(const double)` would also work. For brevity, a discussion on the
const-ness of arguments is deferred to later and simple pass-by-copy signatures
are discussed for now.

There are to functions above because `real` in Stan can correspond to either a
`double` or a `var`. The simplest way to implement this function would be to
define in `prim/fun` the generic signature:

```cpp
template <typename T>
T sin(T);
```

Assuming the implementation is compatible with both `double` and `var`, this is
all that is needed.

A function `real norm(vector)` similarly requires the following C++ functions
to be defined:

```cpp
double norm(Eigen::VectorXd);
var norm(Eigen::Matrix<var, Eigen::Dynamic, 1>);
```

In this case, there is no generic `T norm(T)` function to write because the
input argument is actually different than output type. To handle this, Stan
provides the template metaprogram `return_type_t`. For a given template
argument `T`, `return_type_t` computes the scalar autodiff return type.

- If `T` is `double`, `return_type_t<T>` is `double`
- If `T` is `var`, `return_type_t<T>` is `double`
- If `T` is `Eigen::Matrix<var, Eigen::Dynamic, 1>`, `return_type_t<T>` is `double`
- If `T` is `std::vector<var>`, `return_type_t<T>` is `var`

`return_type_t` is defined recursively to always return the innermost scalar
types. This means:

- If `T` is `std::vector<S>`, `return_type_t<T>` is `return_type_t<S>`

With this in mind, the appropriate generic C++ implementation for
`real norm(vector)` is:

```cpp
template <typename T>
return_type_t<T> norm(Eigen::Matrix<T, Eigen::Dynamic, 1>);
```

### Two Argument Functions (and least-upper-bound return type semantics)

Multiple argument functions are more involved. Consider
`real atan2(real, real)` which requires four C++ overloads:

```cpp
double atan2(double, double);
var atan2(double, var);
var atan2(var, double);
var atan2(var, var);
```

In this case, the return types are a function of both input types.
`return_type_t` is actually written to take any number of input types and
compute from them a single scalar return type. The return type is the simplest
type that can be constructed from all of the input types. For reverse mode
autodiff, this means that if the scalar type of any input argument is a
`var`, then the return type is a var.

A generic signature for `real atan2(real, real)` is:

```cpp
template <typename T, typename S>
return_type_t<T, S> atan2(T, S);
```

`return_type_t` can be used to construct non-scalar return types as well.
For the function `vector add(vector, vector)`, the following generic
C++ signature could be used:

```
template <typename T, typename S>
Eigen::Matrix<return_type_t<T, S>, Eigen::Dynamic, 1> atan2(T, S);
```

### Higher Order Autodiff

Adding the forward mode autodiff type simply adds more overloads. The forward mode
autodiff type itself takes a template argument to represent the types of the
value and gradient and allows recursive templating. This technically defines
an infinite number of new types, but the ones of interest in Math (and the
ones that should be tested are) `fvar<double>`, `fvar<fvar<double>>`,
`fvar<var>` and `fvar<fvar<var>>`. These are useful for computing various
high order derivatives. Going back to the `real sin(real)` function, Math
is now expected to implement the following, rather expanded list of functions:

```cpp
double sin(double);
var sin(var);
fvar<double> sin(fvar<double>);
fvar<fvar<double>> sin(fvar<fvar<double>>);
fvar<var> sin(fvar<var>);
fvar<fvar<var>> sin(fvar<fvar<var>>);
```

For the sake of performance, it may be desirable to also define `var sin(var)`,
or similarly `fvar<double> sin(fvar<double>)` etc.

`return_type_t` is defined similarly as for `var`. In general, autodiff types
should not be mixed, and so the `return_type_t` does not need to account
for various combinations of `var`, `fvar<double>`, etc. Sometimes it is
useful to mix autodiff types, but it is somewhat uncommon.

### Eigen Expressions (and handling argument types with SFINAE)

An additional complexity of the Math library is that `vector`, `row_vector`,
and `matrix` do not map to just templated `Eigen::Matrix` types, they can
also map to a variety of Eigen expressions.

For instance, in the following code, the result `c` is not a vector, but
a vector-like type representing the sum of `a` and `b`:

```cpp
Eigen::VectorXd a(5);
Eigen::VectorXd b(5);
auto c = a + b;
```

Similarly, there are other representations of vectors and matrices
yet discussed, in particular matrices stored in Math's special arena
memory or matrices stored on a GPU. In either case, it is expedient
to not write explicit overloads for all the different types that a
function might accept, but limit them with template meta-programs.

For instance, if only base Eigen types are allowed, then a function
that takes vector types could be defined as follows:

```cpp
template <typename T>
T norm(const Eigen::Matrix<T, Eigen::Dynamic, 1>&);
```

A typical problem with a function like this is that `norm` can
similarly be defined for a row vector (and the implementation is
the same). In this case there are two signatures:

```cpp
template <typename T>
T norm(const Eigen::Matrix<T, Eigen::Dynamic, 1>&);
template <typename T>
T norm(const Eigen::Matrix<T, 1, Eigen::Dynamic>&);
```

The simple solution to this problem is to move to a fully templated signature:

```cpp
template <typename T>
return_type_t<T> norm(const T&);
```

The immediate problem here is what if there actually are differences in
the implementations? The most common situation is that encountered when
there is one generic templated implementation that works with any autodiff
type, and then a second faster overload that only works with a specific
autodiff type.

There can easily be ambiguities between the two function signatures. The
previous examples took advantage of overloads. If a function overload
is more specialized than a template, then the C++ compiler favors the
overload. This only works in simple cases though. The more general solution
are template metaprograms that take advantage of C++ substitution failure
is not an error (SFINAE) semantics. For instance, for the norm function above,
SFINAE could be used to limit one signature to work with reverse
mode autodiff types and one to work with anything else:

```cpp
template <typename T,
          require_st_var<T>* = nullptr>
return_type_t<T> norm(const T&);
template <typename T,
          require_not_st_var<T>* = nullptr>
return_type_t<T> norm(const T&);
```

SFINAE should be thought of as filters on what functions are visible to the
compiler when it does name lookup for a specific function. `require_st_var`
should be read "require the scalar type of the argument to be a `var`". The
meta-program `require_st_var<T>` will evaluate to a valid type if the scalar
type of `T` is a `var`. If the scalar type of `T` is not a `var`, then
`require_st_var<T>` does not evaluate to a valid type, and it is not possible
to do template substitution on this signature, and the compiler treats it as
if it does not exist. This is how SFINAE (substitution failure is not an error)
works. Because the substitution does not work, the signature is ignored.
This is all built similarly to C++'s
[std::enable_if](https://en.cppreference.com/w/cpp/types/enable_if)
template metaprogram.

Again, there are many ways to solve a problem in C++. In particular there
are cases where simple overloads or template specializations can achieve the
same thing as the SFINAE template meta-programs. Unless there is a specific
reason though, new functions in Math should use the SFINAE meta-programs to
handle different implementations. These are tested to work with the broad
set of C++ types that the relatively compact set of Stan types map to.

The vast majority of the SFINAE template meta-programs in Math are special
to Stan or Math. Documentation can be found in the
[Doxygen](https://mc-stan.org/math/) docs. If functionality is not available,
it new meta-programs should be added.

## Prim (fully templated)

### Exceptions

Math functions are expected to validate inputs and throw exceptions.
It should be impossible to cause a segfault by calling a
user-facing Math function or trigger an exception that is not handled
by Stan itself. This means that argument sizes should be checked to be
compatible. For instance, a matrix multiply function would need to check
that the first matrix can in fact be multiplied by the second, if this
is not so it should throw.

If a scalar argument should be positive, an exception should be thrown
if it is not. If a matrix argument should be positive definite, the
function should check this before operating on the matrix. This can
lead to significant overhead in the checks, but the current library
policy is to get informative errors back to the user, which means doing
these checks.

Error checks should be done with the functions in
[stan/math/prim/err](https://github.com/stan-dev/math/tree/develop/stan/math/prim/err)
which throw consistently formatted errors.

### Reference types

Any time a function takes a vector or matrix type, it needs to be able
to handle an expression. Some expressions can be expensive to evaluate, so
when they are evaluated, they should be saved. This includes any arithmetic
operation, a matrix multiply, or a sum, or something else. Some expressions
are cheap to evaluate though, any of the Eigen views quality here (transpose,
block access, segment, etc). In this case, evaluating the expression is not
necessary -- it would only lead to another allocate and copy.

The Math function `to_ref` is a solution for this. For a vector
or matrix variable `x`, `to_ref(x)` returns an Eigen reference to the
input variable `x`. If `x` was an expensive expression, it will be evaluated.
If it was a cheap expression, the reference type won't evaluate. If it
isn't an Eigen type, the `to_ref` just forwards.

```cpp
template <typename T,
          require_eigen_t<T>* = nullptr>
auto myfunc(const T& x) {
  const auto& x_ref = to_ref(x);
}
```

### Holders

If a function returns an Eigen expression, it may be necessary to use
the `make_holder` utility.

Returning expressions is tricky because the expression may have a longer
lifetime than the objects it operates on. For instance, returning an
expression on a local variable leads to this situation:

```cpp
template <typename T>
auto add_zero(const T& a) {
  Eigen::VectorXd b = Eigen::VectorXd(a.size());
  return a + b;
}
```

Because the return type of this function is `auto`, it is possible
that it returns an Eigen expression.

The following code will segfault on the construction of `c` because
by this time the temporary `b` will have been destructed.

```
Eigen::VectorXd a(5);
Eigen::VectorXd c = add_zero(a);
```

`make_holder` can be used to extend the scope of `b` so that it does
not immediately get destructed.

```cpp
template <typename T>
auto add_zero(const T& a) {
  Eigen::VectorXd b = Eigen::VectorXd(a.size());
  return make_holder([](const auto& l, const auto& r) { return l + r; }, std::move(b));
}
```

Returning expressions is an advanced feature and it is easy to make
mistakes. In this regard, it is simplest to start development not
returning expressions (in this case holders are unnecessary), and only
add expression return types later.

It is always possible to return a non-expression type by evaluating
the Eigen expression. For convenience, there is an `eval` function
in Math that will evaluate Eigen expressions and forward anything
else. This is convenient because it works on non-Eigen types as well
(as compared to the built in `.eval()` member function on Eigen
types).

The implementation of `make_holder` is
[here](https://github.com/stan-dev/math/blob/develop/stan/math/prim/meta/holder.hpp).

### Tests

Tests in Math are written with the Google test framework. At this
point there are enough functions in Math that the way to get started
is by just copying the tests of a similar Function and editing them to
suit the new function. There are two basic sets of functions that every
Stan function should have. First, for a function named `foo`, there
should be a file in `test/unit/math/prim/fun` named `foo_test.cpp`.

This should include tests that:

1. Check that every exception is throwing in the right way at least once
and that all edge cases are handled correctly.

  For instance, if there is a `check_size_match` function, then there
  should be a test that makes sure that if the sizes don't match, this
  error gets thrown. It is not necessary to check the error message in
  the tests, just check that an exception is thrown and the exception
  type is correct.

2. Check that the function returns the expect values for a couple
suitable inputs.

  For some functions these checks should be more extensive that others,
  but there should at least be a two or three even in simple cases.

The `prim` tests should only have scalar arithmetic types. No autodiff
is tested here. The `prim` tests check that the function handles edge
cases correctly and produces the correct output for different function
values. The second set of tests should be in `stan/math/mix/fun` with
the same name `foo_test.cpp`. These tests will be used to test all
forms of the autodiff.

The tests in `mix/fun` should use the `expect_ad` function to test
all forms of autodiff. `expect_ad` takes as its first argument a
function to test and then up to three other arguments, all with
arithmetic scalar types. For a given function `f`, and two arguments
`a` and `b`, `expect_ad` tests that:

1. For every combination of `a` and `b`, the returned value of `f`
matches the returned value with non-autodiff types.

2. The finite difference gradients of `f` match those computed with
autodiff.

3. Any time a `f` throws with arithmetic arguments, it also throws
with autodiff arguments.

## Reverse Mode

### Arena memory

Reverse mode autodiff in Math requires a huge number of temporaries and
intermediates to be computed and saved for the reverse pass. There are
so many allocations that the overhead of `malloc` becomes noticable. To
avoid this, Math provides its own memory arena. The assumptions of
the Math arena are:

1. Individual objects allocated in the arena do not need their destructors
called
2. All objects allocated on the arena have the same lifetime (that is the
lifetime of the stack)

Because of this, the Math arena is very easy to implement and is very
high performance (it is a `malloc` that never needs to free).

Functions that use reverse mode autodiff are allowed to save whatever they
need to the arena. Objects saved here will be available during the reverse
pass.

There is a utility for saving objects into the arena that do need
their destructors called. This is discussed undere `make_callback_ptr`.

### Arena types

Arena types are helper functions and types for easily saving variables to
the arena.

`to_arena` is a function that takes one argument (usually an Eigen vector
or matrix) and returns a lightweight object pointing to memory on the
arena.

`arena_t<T>` defines, for a given type `T`, the type of a lightweight
object pointing to memory in the arena that can store a `T`.

Copying a variable `x` to the arena can be done with either `to_arena` or
`arena_t<T>`:

```cpp
template <typename T>
auto myfunc(const T& x) {
  auto arena_x = to_arena(x);
}
```

```cpp
template <typename T>
auto myfunc(const T& x) {
  arena_t<T> arena_x = x;
}
```

### Array of Structures vs. Structure of Array types (AOS vs. SOA, matvar vs. varmat)

There are two types of reverse mode autodiff in Math. They are colloquially
referred to as matvar and varmat, though it is more accurate to
refer to them as Array of Structure (AOS) and Structure of Array (SOA)
autodiff types.

The first autodiff type in Stan was `var`. A `var` is the autodiff version
of a `double`. Constructing matrix autodiff types with `var` is as simple
as building an Eigen matrix and filling it with vars, so
`Eigen::Matrix<var, Eigen::Dynamic, Eigen::Dynamic>`. This is an AOS
autodiff style.

The SOA autodiff style defines vector and matrices themselves as autodiff
variables. The SOA matrix is `var_value<Eigen::MatrixXd>`. Defining an
entire matrix as an autodiff type makes it possible to do a number of
optimizations that are not possible when the autodiff types are just
variables stored inside another structure.

### Reverse mode `require` metaprograms

Require metaprograms are useful for avoiding ambiguities between fully
templated prim implementations and reverse mode specializations.

The simplest way to do this is add a `require_any_st_var<...>` to
the reverse mode implementation and a `require_all_not_st_var<...>` to
the prim implementation.

### Values

`value_of(x)` returns the values of the variable `x`. If `x` is not an
autodiff type, it simply forwards `x` along. The values of `x` have
the same shape as `x`. For reverse mode autodiff, values are `double`
variables. For higher order autodiff (`fvar<var>`, etc.) this is not
always the case. See `value_of_rec` for a different behavior.

### Values and adjoint extensions to Eigen

The AOS matrix and vector autodiff types come with an extra `.val()`,
`.val_op()`, `.adj()`, and `.adj_op()` member functions.

`.val()` and `.val_op()` return expressions that evaluate to the values
of the autodiff matrix.

`.adj()` and `.adj_op()` return expressions that evaluate to the adjoints
of the autodiff matrix.

The non-`_op` versions of the functions should be preferred. If the
code does not compile (usually an error about const-ness) try the `_op`
versions. This can happen when multiplying the values/adjoints of an AOS
autodiff type against other matrices.

It is not safe to use `.noalias()` with the AOS value/adjoint expressions.

These functions also work for SOA types, but return arena types instead.

### Implementing the reverse pass

#### `reverse_pass_callback`

`reverse_pass_callback` is a function for adding code to the reverse pass.
Calling `reverse_pass_callback` with a functor `f` creates an object on the
reverse mode stack that will call `f` when it is chain'd.

#### `make_callback_var`

`make_callback_var(x, f)` is similar to `reverse_pass_callback`, but it constructs
an SOA autodiff type from the argument `x`. The function `f` is called on the
reverse pass similarly to `reverse_pass_callback`.

### Multiple argument scalar types

Pretend for a minute that 

### Tests (SOA types)

SOA tests are separate from the regular AOS tests. To test SOA types, add for
every `expect_ad` call an `expect_ad_matvar` call. `expect_ad` builds on the
`prim` tests by using finite derivatives to check all of the higher order
autodiff. `expect_ad_matvar` builds on the `expect_ad` tests by checking the
SOA autodiff agains the AOS autodiff.

## Pitfalls

### Copying non-arena variables to lambdas used in the reverse pass

### Returning arena types

### `.val()` vs. `.val_op()`

### Returning expressions

### Const correctness, reverse mode autodiff, and arena types

## Handy tricks

### `make_callback_ptr`

### `forward_as`

### Nested Stacks

### Scoped Stacks

As an aside, in order to get the actual vector result, Eigen defines the
`.eval()` operator, so `c.eval()` would return an actual `Eigen::VectorXd`.
Similarly, Math defines the `eval` function which will evaluate an Eigen
expression and forward anything that is not an Eigen expression.

### `value_of_rec`